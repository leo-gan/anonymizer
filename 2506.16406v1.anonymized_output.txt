## **Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights**

**PERSON_1** [1] _[∗]_ **PERSON_2** [1] **PERSON_3** [1] **PERSON_4** [1] **PERSON_5** [1]

**PERSON_6** [1] **PERSON_7** [1] **PERSON_8** [2] **PERSON_9** [3] **PERSON_10** [3]

**PERSON_11** [4] **PERSON_12** [1] **PERSON_13** [2] _[∗]_ **PERSON_14** [1] _[∗]_

1 ORGANIZATION_1, 2 ORGANIZATION_2, 3 ORGANIZATION_3, 4 ORGANIZATION_4


**Abstract**


Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank adaptation (LoRA) reduce the cost of customizing large language models (LLMs), yet
still require a separate optimization run for every downstream dataset. We introduce **Drag-and-Drop LLMs (** _**DnD**_ **)**, a prompt-conditioned parameter generator
that eliminates per-task training by mapping a handful of unlabeled task prompts
directly to LoRA weight updates. A lightweight text encoder distills each prompt
batch into condition embeddings, which are then transformed by a cascaded hyperconvolutional decoder into the full set of LoRA matrices. Once trained in a diverse
collection of prompt-checkpoint pairs, DnD produces task-specific parameters in
seconds, yielding i) up to **12,000** _×_ lower overhead than full fine-tuning, ii) average
gains up to **30%** in performance over the strongest training LoRAs on unseen
common-sense reasoning, math, coding, and multimodal benchmarks, and iii)
robust cross-domain generalization despite never seeing the target data or labels.
Our results demonstrate that prompt-conditioned parameter generation is a viable
alternative to gradient-based adaptation for rapidly specializing LLMs. Our project
[is available at URL_1]


**1** **Introduction**


Large Language Models (LLMs) such as GPT-4, Llama 2/3, Qwen2.5, and DeepSeek have rapidly
become the backbone of contemporary natural-language processing and artificial intelligence more
broadly, thanks to their Internet-scale pre-training and transformer architectures [ 1, 18, 62, 35 ]. This
pre-training endows a single model with broad _zero-shot_ competence across mathematics, coding,
reasoning, and even multimodal understanding [ 23, 11, 61, 28 ]. Yet, real-world deployments rarely
stop at zero-shot use; they demand _task-specific_ behavior that reflects internal data, domain jargon,
or bespoke response styles. Parameter-Efficient Fine-Tuning (PEFT) aims to satisfy this demand
by inserting a small set of trainable weights, most prominently the low-rank adapters of LoRA [ 24 ].
While LoRA allows to maintain the number of trainable parameters and storage overhead small by
keeping the model frozen, the _wall-clock cost_ remains very high: for example, adapting the lightest
0.5 B-parameter Qwen2.5 with LoRA still occupies four A100 GPUs for half a day [ 62 ]. Moreover,
each downstream user or dataset requires its own optimization run, which quickly becomes the
computational bottleneck for practitioners when deploying PEFT at massive scales.


We observe that a LoRA adapter is nothing more than a _function of its training data_ : gradient
descent “drags” the base weights towards a task-specific optimum (Figure 1). If that mapping from
_prompts_ to _weights_ can be learned _directly_, we could bypass gradient descent altogether. Early
work on _parameter generation_ has shown that hyper-networks can synthesize billions of weights in
minutes [ 50, 42, 57, 48, 52 ]. Yet they either ignore task conditioning or use simple binary embeddings.


  - PERSON_1.v_2, PERSON_13.v_2, and PERSON_14.v_2 are core contributors.


Preprint. Under review.


(b) Drag-and-Drop



(a) Parameter-Efficient-Fine-Tuning



Figure 1: _Left_ : Parameter-efficient methods such as LoRA need _hours_ to optimize LLMs in order to
adapt them to novel datasets. _Right_ : Our method adapts LLMs by directly generating LoRA matrices
for novel datasets in _seconds_ **without any tuning** .


Recent progress makes this goal attainable. RPG [ 58 ] is one of the first approaches to condition on
task information and generate an entire classifier in a single pass, matching from-scratch training on
previously unseen image classes in zero-shot. Translating that success to language, however, raises
new obstacles. First, linguistic prompts carry orders of magnitude more semantic variation than the
binary embeddings used by RPG. A practical generator must therefore ingest _rich task descriptors_
and preserve their nuances. Second, an LLM in production may face hundreds of heterogeneous
workloads, so the conditioning mechanism must scale gracefully while injecting task-specific cues
with high fidelity. These challenges bring the need for a _compact yet expressive_ representation that
both captures salient features of the input texts and steers the hyper-network towards the corresponding
region of LoRA weight space. Designing such a representation is the central challenge that our
method, introduced next, is built to address.


We introduce **Drag-and-Drop LLMs (DnD)**, a prompt-conditioned hyper-generator that converts a
handful of unlabeled task prompts into a complete set of LoRA adapters in seconds, eliminating any
per-task optimization. DnD employs an off-the-shelf, lightweight text encoder to compress a given
batch of prompts into conditional embeddings, which a cascaded hyper-convolutional decoder then
expands into LoRA updates for every transformer layer.


On common-sense reasoning, mathematics, code-generation, and multimodal benchmarks, DnD
cuts adaptation overhead by up to _12,000_ _×_ while yielding up to _30%_ increased performance on
_unseen_ datasets compared with the strongest training LoRAs, and transfers seamlessly from 0.5B
to 7B parameter backbones. By collapsing the classical “data _→_ gradients _→_ weights” loop into a
single forward step, DnD challenges the notion that gradient descent is indispensable for model
specialization and opens a new path where weights themselves become a new data modality and
generative target conditioned on concise task descriptors.


Our main contributions are outlined as follows:


**New LLM adaptation paradigm.** We cast LoRA adaptation as direct generation of task-specific
weights from raw prompts in a _novel_ dataset and realize this mapping with a scalable hyper-generator,
which is way more efficient than traditional tuning.


**Practical Architecture.** A frozen text encoder coupled with a hyper-convolutional decoder is able to
generate large scale parameters while reducing adaptation overhead by four orders of magnitude.


**Comprehensive evaluation.** Experiments across reasoning, math, coding, and multimodal show
up to 30% zero-shot gains on unseen datasets and smooth transfer across model sizes, highlighting
DnD’s impressive efficiency and versatility.


**2** **Drag-and-Drop Your LLMs**


**2.1** **Preliminary**


**Parameter-Efficient Fine-Tuning.** Parameter-Efficient Fine-Tuning (PEFT) saves training costs via
introducing and tuning only a small number of additional parameters while keeping the original model
weights frozen. This approach has been applied to LLMs and other foundation models, particularly
in Low-Rank Adaptation (LoRA) manner, such as LLaMA [ 18 ] and Stable Diffusion [ 47 ]. The
optimization process can be formulated as:


2


|datasets
Rossetta Alpaca ShareGPT|Col2|
|---|---|
|Rossetta|ShareGPT|

|checkpoints
LLM LLM LLM
Rossetta Alpaca ShareGPT|Col2|
|---|---|
|LLMRossetta|LLMShareGPT|




Figure 2: Our approach obtains dragging ability via two processes: prepare training data ( _upper_
_left_ ) and training the parameter generator ( _upper right_ ). When preparing training data, we explicitly
pair parameters with dataset-specific conditions. During training, DnD takes condition as input and
generate parameters, using original parameters as supervision.

min _A,B_ _[L]_ [(] _[W]_ [0] [ +] _[ BA,][ D]_ [)] _[,]_ (1)


where _W_ 0 is the frozen original weight matrix, low-rank matrices _B ∈_ R _[d][×][r]_ and _A ∈_ R _[r][×][k]_ with
_r ≪_ min( _d, k_ ) are the only trainable parameters and _D_ represents the fine-tuning dataset. Based on
Equation 1, we can conclude that LoRA uses data _D_ as raw material and optimization as the driving
force to obtain weight space shift ∆ _W_ = _BA_, thus making ∆ _W_ and _D_ strongly associated.
**Parameter generation.** This approach treats models or trained weights as data, aiming to synthesize
high-performing neural network parameters without conventional training. Recent advances, such as
COND P-DIFF [ 26 ], RPG [ 58 ], SANE [ 52 ], and ORAL [ 27 ] have achieved controllable generation
by incorporating conditioning mechanisms, allowing primitive personalized generation of parameters
for simple datasets. The parameter generation process shares fundamental commonalities with PEFT,
where conditions serve as raw materials and the parameter generator provides the driving force to
produce target weights with specific properties.


One questions remains: Can we utilize parameter generation to effectively “drag-and-drop” LLMs’
weights towards configurations better suited for a given novel task? By “drag-and-drop”, we draw an
analogy to our simple, _tuning-free_ process that directly generates _task-specific weights_, associating it
to the intuitive action of dragging a file and dropping it into place without further configuration.
**Key challenges.** Before addressing the above question, we analyze the potential challenges.


**Challenge 1:** How to equip the parameter generator with effective “drag-and-drop” ability? The
generator should produce parameters that can effectively adapt LLMs towards specific tasks.


**Challenge 2:** How to enable adaptation without task-specific training? Traditional PEFT methods
typically require training on new tasks, but can we achieve comparable performance by directly
generating parameters without any fine-tuning on the target task?


**Challenge 3:** How to make the drag-and-drop function user-friendly and accessible? The generation
mechanism should be simple and intuitive, enabling broader adoption and practical deployment.


**2.2** **Overview of DnD**


To address the identified challenges, we present DnD as illustrated in Figure 2. As preparation, we
first train and save LoRA adapters on various datasets. To develop the “drag-and-drop” capability,
our approach should be aware of parameters’ correlations with datasets. Consequently, we randomly
pair collected checkpoints with prompt batches from their training data. A pre-trained text encoder
then extracts embeddings from the prompts and feed them to our parameter generator. The generator
features a simple architecture of cascaded pure convolutional decoder blocks (details in Section 2.5
and Appendix A.3). We optimize this generator using mean squared error (MSE) loss between the
generated and original tokenized model weights. During inference, we evaluate our approach in both


3

in-domain and cross-domain scenarios: simply feeding prompts from novel datasets (not seen during
training) to DnD to obtain tailored model parameters with one single forward pass.


**2.3** **Data Preparation of DnD.**


**Checkpoint collection.** We collect the checkpoints across various datasets, _i.e._, serving as diverse
supervisions, to equip the capability of DnD. The collection process follows previous parameter
generation works [ 57, 58 ]: training for specified epochs, then performing iterative fine-tuning while
preserving checkpoints at each iteration (more details in Appendix A.1).
**The role of prompts.** Recent studies [ 37, 65 ] demonstrate that samples from different datasets
exhibit distinct features, _i.e._, samples could be considered as “fingerprints” of specified datasets
(tasks). Based on this observation, we utilize data samples (prompts) as representative proxies for
their respective datasets (tasks). To establish data-parameter mapping, we incorporate prompts from
the datasets used to train these checkpoints. These prompts contain dataset-specific features, enabling
the generator to infer appropriate “dragging” directions for models across various tasks.
**Prompt-checkpoint pairing.** Based on the above analysis, the next important question is: how to
utilize these elements to equip DnD with “drag-and-drop” ability in training? Given a dataset _P_, we
first divide it into non-overlapping prompt batches [ _p_ 1 _, · · ·, p_ _i_ _, · · ·, p_ _I_ ] . We note the trained LLM
checkpoints of this dataset as _M_ = [ _m_ 1 _, · · ·, m_ _j_ _, · · ·, m_ _J_ ] . We randomly pick a batch of prompts
and a corresponding checkpoint. The process can be formulated as,


randomly pick randomly pick

[ _p_ 1 _, · · ·, p_ _i_ _, · · ·, p_ _I_ ] _−−−−−−−→{p_ _i_ _, m_ _j_ _}_ _←−−−−−−−_ [ _m_ 1 _, · · ·, m_ _j_ _, · · ·, m_ _J_ ] _,,_ (2)


where _{p_ _i_ _, m_ _j_ _}_ serves as a pair for parameter generator training. Prompt _p_ _i_ and checkpoint _m_ _j_ serve
as the input and supervision, respectively.


**2.4** **Prompt Embedding.**


For each batch of prompts, we employ a open-sourced text encoder to extract embeddings that serve
as inputs for the parameter generator. This extraction process can be formally represented as:

_c_ _i_ = Encoder( _p_ _i_ _, θ_ ) _,,_ (3)


where Encoder( _·, ·_ ) denotes the embedding extraction function parameterized by _θ_, and _c_ _i_ represents
the extracted embedding corresponding to prompt _p_ _i_ . By default, we leverage an encoder-based
language model architecture [ 13 ] for prompt embedding. In the experimental section, we further
explore and quantitatively evaluate alternative embedding approaches, including word2vec representations [43], encoder-decoder architecture [45], and decoder-only language models [62].


**2.5** **Training and Inference of DnD.**


Figure 3: Block details of parameter generator. Each block of hyper-convolution contains
three hyper-convolution modules, extracting
and fusing features in different dimensions.
More details are in Appendix A.3.


**Structure of parameter generator.** Different from diffusion-based parameter generation [ 57, 26, 58 ],
we use a hyper-convolutional decoder to learn the mapping between input prompts and parameters.
That design mainly considers efficiency, as the decoder-only structure has shown its superiority
in LLMs [ 1, 18, 62, 21 ]. We show the block details of the decoder in the right part (Figure 3).
We assume the dimension of input prompt embeddings is [ _B, N, L, C_ ], where _B_, _N_, _L_, and _C_
denote batch size, length of prompt batch ( _i.e._, number of prompts), sequence length, and hidden
dimension, respectively. The cascaded convolutional blocks transform prompt embeddings to match
the dimensions of tokenized weights. Here, we refer the output of the last block as [ _B, N_ _w_ _, L_ _w_ _, C_ _w_ ] .
**Learning objective.** The learning objective is simple: we calculate the mean squared error (MSE)
loss between the output of the last block of the parameter generator and the corresponding tokenized
checkpoints. Similar to RPG [ 58 ], we tokenize each layer’s parameters into non-overlapping segments
and apply padding, ensuring checkpoints have a consistent shape of [ _B, N_ _w_ _, L_ _w_ _, C_ _w_ ] . Formally, we
write the MSE loss below,


4


parameter generator tokenization [ 58 ]
prompt embeddings _−−−−−−−−−−→_ _L_ _MSE_ _←−−−−−−−−−_ corresponding checkpoints _._ (4)


**Inference.** We expect the parameter generator to develop effective “drag-and-drop” ability, particularly for novel datasets or tasks not encountered during training. Therefore, our evaluation primarily
focuses on performance across novel datasets. The inference process consists of four steps: 1)
sampling prompts from novel datasets, 2) extracting embeddings from these prompts, 3) feeding the
embeddings into the parameter generator, and 4) evaluating the generated parameters on the novel
datasets. To comprehensively demonstrate the “drag-and-drop” ability of our approach, we examine
performance in both in-domain scenarios ( _e.g._, common sense-to-common sense) and cross-domain
scenarios ( _e.g._, common sense-to-science).


**3** **Experiments**


**3.1** **Implementation Details**


We choose Qwen2.5 [ 62 ] series as foundation model and conduct experiments on common sense
reasoning, coding, math, and multimodal tasks. The details of involved model sizes and datasets for
each task are listed in the table below. The default text encoder is Sentence-BERT [ 46 ], and length of
prompt batch is set to 128, 64, 64 and 32 for common sense reasoning, math, coding, and multimodal
task, respectively. For other hyper-parameter settings, please refer to Appendix A.1.

task #model size (B) datasets


**3.2** **Common Sense Reasoning**


**Evaluating setting.** We employ LoRA [ 24 ] to fine-tune Qwen2.5-0.5B on seven common sense
reasoning datasets and save the checkpoints as training data. In every column of Table 1, we use the
specified dataset as test set ( _i.e._, not used in training) and train DnD on other datasets’ LoRAs.

|method \ test set|ARC-e|OBQA|ARC-c|PIQA|HellaSwag|BoolQ|WinoGrande|
|---|---|---|---|---|---|---|---|
|training LoRAs<br>**DnD**|37.5<br>**68.6**|30.2<br>**40.8**|39.5<br>**51.6**|40.5<br>**87.9**|22.4<br>**25.9**|13.5<br>**44.9**|38.8<br>**50.0**|



average accuracy improvement: 21.0 on training LoRAs


Table 1: **Generalization on novel (test) datasets** . Our approach significantly outperforms LoRAs
used in training in terms of accuracy (%) across _all_ unseen datasets. **Bold entries** are the best results.


**Analysis.** We report the average accuracy of training LoRAs and our generated ones in Table 1.
Several observations can be made from these results: i) Our method consistently outperforms LoRAs
used for training on unseen datasets, indicating it manages to drag-and-drop LLM parameters to
task-specific distribution specified via condition. ii) This drag-and-drop ability holds across different
datasets, showing strong robustness towards various data inspirations.

**Cross-domain Drag-and-Drop.** To further explore DnD’s zero-shot ability, we not only use
in-domain novel datasets in inference, but also

test it on cross-domain task. In this part, we Table 2: Our approach also succeeds on crossuse the checkpoint trained on common sense domain scenario ( _i.e._, both novel data and task).
reasoning tasks and feed it with prompts from
science-dataset [ 33 ]. The generated parameters are compared with training LoRAs on the science
dataset. From Table 2, we can observe that DnD surpasses its training LoRAs’ average accuracy,


5


indicating our method manages to drag-and-drop LLMs for cross-domain tasks ( _i.e._, from common
sense reasoning to science).


Table 3: DnD can generate parameters for more complex tasks like math, code and multimodal
question answering. Our method continues to show strong zero-shot ability on these tasks.


**3.3** **Generalization to coding, math, and multimodal tasks**


To further validate our method’s applicability in more complex scenarios, we also employ DnD for
coding, math, and multimodal tasks. The empirical results and findings are as belows.
**Coding.** Similar to common sense reasoning task, we use LoRA to fine-tune Qwen2.5-1.5B on seven
coding datasets and save the checkpoints as training data. Evaluation is carried out on HumanEval [ 8 ]
benchmark using pass@k [ 32 ] score (k = 1, 5, 10). _Note that neither LoRA fine-tuned models nor_
_DnD has seen any samples of the benchmark in their training_ . Therefore, we directly test training
LoRAs and our synthesized ones on HumanEval. From Table 3, we can draw some findings: i) Our
method yields promising results, with improvement over average **pass@1 = 15.1, pass@5 = 26.7,**
**pass@10 = 30.9** . ii) Despite training LoRAs perform poorly on the test set, DnD still obtains good
performance. _This means instead of memorizing parameters seen in training, it learns to fit novel_
_datasets given condition as inspirations_ .
**Math.** We fine-tune Qwen2.5-1.5B on six math datasets and save the checkpoints. We adopt
gsm8K [ 11 ] and MATH [ 23 ] as our benchmarks and accuracy as evaluation metrics. Results listed
in Table 3 show commonalities with those regarding common sense reasoning and coding tasks,
underscoring the superiority of our method across a wide range of scenarios.
**Multimodal.** The above results confirm our method’s effectiveness in text modality. In the following,
we explore its greater potential by pacing towards other modalities. We fine-tune Qwen2.5-VL-3B [ 4 ]
on MathV360K [ 54 ], save the checkpoints, and evaluate using Math-Vision [ 59 ] and Math-Vista [ 38 ].
Results in Table 3 show that DnD perform well in multimodal tasks, revealing that _our method can_
_be adapted to modalities other than texts_ and has promising application potential.


**Takeaway:** Based on the above results and comparisons, DnD is a _high-performing zero-shot learner_
with strong robustness and wide applicability, as reflected by the _significant improvements_ compared
to its training data and its promising performance _across various scenarios_ . In the following, we
continue to explore more interesting features of our proposed approach.


**3.4** **Ablation Studies**


This section mainly aims at exploring a series of interesting features about our approach. For those
exploring various settings in our experiment, we report them in Appendix B.3 for thoroughness. If
not stated, we use ARC-c as the test set and other common sense reasoning datasets for training.
**What types of data will help Drag-and-Drop LLMs better?** As introduced in Section 2.3, we use
prompts as conditions to inspire DnD. Can this drag-and-drop ability holds when condition types
changes ( _e.g._, answers)? We carry out ablation studies by changing condition types as prompt, prompt
+ answer and their mixture (prompt : answer=4 : 1) and report the results in Table 4a.


It can be observed that the prompt + answer group surprisingly lead to poor performance. We
conclude that it is because answers in common sense reasoning datasets lack diversity ( _i.e._, A/B/C/D)
and combining them with prompts may detriment dataset-specific representations. This shall hinder
generator to distinguish different datasets and generate specific parameters. Consequently, we advise
not to use answer alone as inspirations. However, conclusions may be different for some tasks where
answers are more complicated and diverse and we show the results on these tasks in Appendix B.3.
**How does the choice of condition extractor affect DnD’s performance?** Our default condition
extractor is Sentence-BERT [ 46 ], yet it is interesting to explore other models’ potentials. To ensure


6


condition type accuracy

prompt + answer 27.0

mix 49.7



condition extractor accuracy

Glove 50.8

Sentence-BERT **51.6**

T5-base 50.2

Qwen2.5-7B fail


dataset arrangement improves.


4 _∈_ train, 3 _∈_ test 11.4

3 _∈_ train, 4 _∈_ test 0.8

2 _∈_ train, 5 _∈_ test -1.4



(a) **Condition types.** Pure prompts (b) **Extractor structure.** Several (c) **Train-test set arrangements.**
used as inspirations yield the best encoder-based extractors perform More diverse training data introresults compared to other formats. better than decoder-only ones. duces higher improvements.

Table 4: Ablation studies about condition types, condition extractor’s type, and train-test set arrangement. This series of explorations validate several designs of DnD.



(a) **Condition types.** Pure prompts
used as inspirations yield the best
results compared to other formats.



(b) **Extractor structure.** Several
encoder-based extractors perform
better than decoder-only ones.


thorough comparisons, we include classical word2vector method Glove [ 43 ], the default encoder-only
Sentence-BERT [46], encoder-decoder model T5 [45] and decoder-only Qwen2.5-7B [62].


Results in Table 4b reveal several insights: i) Even traditional methods such as Glove can help DnD
to obtain promising result, indicating our method can fit plenty of text encoders. ii) Qwen2.5-7B
performs not as good as expect, which has two possible causes: First, its heaviness limits the number
of conditions paired with parameters per iteration, leading to poor awareness of novel datasets.
Similar conclusions can be drawn from our experiments in Appendix B.3. Second, Qwen2.5-7B’s
decoder-only architecture may constraint conditions’ diversity, since it encodes prompts to answers.
**What property of training data equips our method with drag-and-drop ability?** By default, we
train on several datasets and test on 1 novel dataset. In this part, we explore DnD’s robustness by
shrinking the number of train sets and test on more datasets. Train-test set arrangements are: 6-1,
4-3, 3-4 and 2-5. Generated parameters are compared with training LoRAs’ average accuracy on the
unseen datasets and their average improvements in accuracy are reported in Table 4c.


It can be observed that: i) Generally, more training datasets lead to better performance improvement.
This is expected since more data ensures better coverage of condition-parameter correlations and
lead to better robustness for novel data. ii) DnD fails to drag-and-drop LLMs to novel datasets when
training samples are few. As datasets used for training lessen, the average improvement of DnD drops
accordingly. It hardly improves over training LoRAs for the 2-5 case. We can conclude that basic
amount of training samples are needed for DnD to learn condition-parameter correlations.

**How does DnD’s performance compared with**
**foundation LLMs?** Given massive pretraining
LLMs often take, fine-tuning on a small downstream dataset may detriment their zero-shot per- HumanEval 32.3 **64.1** 31.8
formance on novel test sets. Aware of this phe- gsm8K 64.4 **66.3** 1.9
nomenon, we compare DnD generated weights’ Math-Vision 22.7 **24.3** 1.6
performance with foundation LLMs across all

Table 5: DnD surpasses foundation LLMs across

tasks involved in our experiment. Specifically,

various tasks, showing the “drag-and-drop” effect.

for foundation LLMs, we adopt Qwen2.5-0.5B
for common sense reasoning, 1.5B for math and coding, and Qwen2.5-VL-3B for multimodal task.
Results in Table 5 again show our approach’s superiority: DnD outperforms foundation LLMs across
all tasks. Its “drag-and-drop” ability can generate task-specific parameters, with performance better
than foundation LLMs that go through abundant pretraining.

**3.5** **Open Explorations and Analysis**


**Condition-Parameter Pairing.** In this part, we explore other pairing strategies’ influence on
performance than that introduced in Section 2.3. We test 2 condition pairing strategies:


    - **Strategy 1:** We fix the total number of prompts to be 128, 256, 512, 1024, 2048, 5000 and
use all those prompts to pair with parameters every iteration ( _x ←_ _x_ ).

    - **Strategy 2:** We fix the length of prompt batch to be 128 in every iteration and randomly
picks these prompts from 128, 256, 512, 1024, 2048, 5000 candidate prompts (128 _←_ _x_ ).


Based on the results in Figure 4a, we can draw several conclusions: i) With limited number of
conditions, DnD fails to generalize over novel data, since it can hardly learn comprehensive knowledge
about condition-parameter mapping’s landscape. ii) As number of condition increases, **Strategy 2** ’s
performance skyrockets since DnD is exposed to sufficient condition-parameter pairs. This indicates
**Strategy 2** may help DnD to converge efficiently. iii) **Strategy 1** needs more conditions to reach


7




Table 5: DnD surpasses foundation LLMs across
various tasks, showing the “drag-and-drop” effect.


(a) Random selection and pairing
is better than using the same conditions for all parameters.



(b) DnD can reach comparable or
even better performance than fullshot while being 2.5 _∼_ 12K _×_ faster.




(c) DnD outperforms popular fewshot tuning and ICL before 256
shots while avoiding using answers.



Figure 4: Explorations about DnD’s condition-parameter pairing strategy, compare DnD with stateof-the-art methods, and its superiority over few-shot tuning and in-context learning.



comparable performance as **Strategy 2** . With large number of conditions, **Strategy 1** suffers from
out of memory issues. The same-condition-per-parameter strategy may hinder DnD to associate
conditions with specific datasets. Conclusively, **Strategy 2** is superior than **Strategy 1** both in model
generality, convergence speed, and memory consumption. These conclusions are consistent with
findings in Table 4c: _Diversity of training data equips our method with drag-and-drop ability_ .
**DnD vs full-shot tuning.** In this part, we compare DnD with full-shot tuning in accuracy and overhead.
Specifically, we test ARC-c tuned LoRA’s ( _≈_ 75 iterations for ARC-c, detailed in Appendix A.4)
full-shot performance using ARC-c’s test set. Since LoRA’s performance might improve with more
iterations, we fine-tune LoRA on ARC-c for 300 iterations and test its performance as well. These
results are compared with **zero-shot** DnD in Figure 4b: i) using mild tuned checkpoints for training,
DnD already yields better results than full-shot tuning. This showcases DnD’s impressive **zero-shot**
ability even surpasses **full-shot tuning** . ii) DnD is incredibly efficient, with better performance than
full-shot tuning while being **2500** _×_ faster. Moreover, as training continues, though full-shot prevails,
our method shows minimal performance gap with it while being **12,000** _×_ more efficient.
**Efficiency analysis of DnD.** In additional to full-shot tuning, in-context learning (ICL) and few-shot
tuning (FS) are also popular methods in LLM fine-tuning. In Figure 4c, we conduct experiments
investigating the performance-efficiency trade-off of them. Several observations can be drawn: i)
Both ICL and FS’ results are poor when shots are few, but their overhead rises as shots increase. ii)
DnD can reach better performance than FS and ICL before 256 shots with negligible overhead. iii)
_It is noteworthy that both few-shot and ICL use answers for instructing the LLM_ to obtain better
performance. On the contrary, DnD relies on only 128 unlabeled prompts. Based on the above results,
we anticipate _DnD is a powerful and efficient zero-shot learner_ .

**Scalability of DnD.** In this part, we explore testset \ method training LoRAs **DnD** improves ( _↑_ )
DnD’s scalability. Since common sense reasoning is simple and 0.5B model suffices, we fo- LiveCodeBench 13.0 **33.3** 20.3
cus on math and coding tasks while increasing gsm8K 65.9 **83.1** 17.2

foundation model size from 1.5B to 7B. We use Table 6: DnD scales well with larger 7B foundation
original math and coding datasets in Section 3.3, models, and maintains strong performance in more
evaluate using gsm8K for math and more diffi- complex benchmark LiveCodeBench.
cult benchmark LiveCodeBench [ 25 ] for coding. We report accuracy on math task and pass@1 for
coding in Table 6. It can be observed that: i) DnD consistently surpasses training LoRAs in both
tasks under 7B model setting, underscoring its eminent scalability for larger foundation LLMs. ii)
With more difficult coding benchmark, DnD maintains superior performance with training LoRAs,
_i.e._, improvement over average **pass@1 = 20.3** . It reveals DnD’s capacity for generalizing to more
complex benchmarks, showing promising application potential and robustness.

**Comparisons with previous**
**methods.** We compare our
method with most recent parameter generation method RPG [ 58 ].
We explore both methods’ performance in two scenarios: Close

|sing application potential and robustness. 0 close-set tests base RPG DnD|open-set test|
|---|---|
|0<br>0<br>0<br>0<br><br>67<br>44<br>46<br>81<br>30<br>~~51~~<br>~~73~~<br>~~51~~<br>25<br>4<br>4<br>50<br>69<br>~~51~~<br>64<br>~~38~~<br>~~36~~<br>~~51~~<br>base<br>RPG<br>DnD|~~51~~|
|0<br>0<br>0<br>0<br><br>67<br>44<br>46<br>81<br>30<br>~~51~~<br>~~73~~<br>~~51~~<br>25<br>4<br>4<br>50<br>69<br>~~51~~<br>64<br>~~38~~<br>~~36~~<br>~~51~~<br>base<br>RPG<br>DnD|46<br>21|
|0<br>0<br>0<br>0<br><br>67<br>44<br>46<br>81<br>30<br>~~51~~<br>~~73~~<br>~~51~~<br>25<br>4<br>4<br>50<br>69<br>~~51~~<br>64<br>~~38~~<br>~~36~~<br>~~51~~<br>base<br>RPG<br>DnD||

set generation: generate param- Figure 5: DnD and RPG perform well in most close-set tests.
eters seen in training. Open set However, RPG can hardly generate parameters for novel dataset
generation: generate parameters while DnD still presents strong zero-shot ability on open-set test.
for unseen datasets. Figure 5 shows both methods work well on close set generation, but RPG fails on



Table 6: DnD scales well with larger 7B foundation
models, and maintains strong performance in more
complex benchmark LiveCodeBench.



























Figure 5: DnD and RPG perform well in most close-set tests.
However, RPG can hardly generate parameters for novel dataset
while DnD still presents strong zero-shot ability on open-set test.


8


open set generation, showing that our designs (prompts as conditions, condition-parameter pairing)
have certain robustness and generality towards novel datasets.

**Visualization for the effect of**
**drag-and-drop.** In this part, we
visualize original and generated
parameters in Figure 6. It can
be seen that original parameters
exhibit diverse patterns in the
weight space, forming different
clusters. Moreover, even parameters close to those tuned on the
target dataset ( _i.e._, ARC-c) can
have large performance gap ( _i.e._,
19.1% compared with 40.7%).
After training on these models Figure 6: DnD generates parameters with close distribution to
that have distinct features, DnD original ones in the weight space and promising performance.
can generate parameters for the target dataset in a zero-shot manner. The generated parameters are
close to the original ones in the weight space and with even superior performance than full-shot
tuning ( _i.e._, 51.6% compared with 40.7%). This brings the “drag-and-drop” effect to life.











Figure 6: DnD generates parameters with close distribution to
original ones in the weight space and promising performance.



**4** **Related Works**


**Parameter-Efficient-Fine-Tuning (PEFT).** LLMs’ size rapidly scales up in the 2020s, making
full parameter fine-tuning infeasible. To address this, Low-Rank Adaptation (LoRA) [ 24 ] has
been proposed, leveraging LLMs’ inherent sparsity and substantially reduces fine-tuning costs by
optimizing two low rank matrices instead of the original weights. Multiple variants of LoRA emerge
afterwards, such as DoRA [ 36 ], LoRA+ [ 22 ], VeRA [ 30 ], and DyLoRA [ 56 ]. However, all of them
have one potential flaw: they requires tuning model parameters for every novel dataset, therefore
lacks generality. This can still induce extra costs as model size enlarges and training data increases.
**Parameter generation.** Parameter generation trains on model checkpoints and aims at generate
high-performing parameters, both seen and unseen in training. Previous works [ 6, 19, 29, 16 ] focuses
on learning distributions over the parameters, yet struggle to reconstruct original models’ performance.
With the development of diffusion models, Hyper-Representations [ 50, 51, 53 ] and p-diff [ 57 ], use
the latent diffusion architecture to generate high-performing parameters. Armed with Mamba [ 20 ] and
appropriate tokenization strategy, RPG [ 58 ] can generate 200M of parameters in minutes. Regarding
conditional generation, COND P-DIFF [ 26 ], Tina [ 34 ] and ORAL [ 27 ] explores text-controlled
parameter generation method. RPG even generate parameters for novel datasets on binary embedding
classification task for CIFAR-10 [ 31 ]. However, these methods can hardly keep promising zero-shot
ability on more complex tasks, hindering parameter generation’s greater potential. On the other hand,
our method use prompts in novel datasets as conditions, captures parameters’ relations with datasets
better, and is able to generate competent parameters for novel datasets.


**5** **Discussion and Conclusion**


PEFT offers an efficient solution to reduce costs when customizing LLMs/VLLMs for downstream
tasks, yet it is still expensive when models are large and tasks are diverse. In our study, we train a
parameter generator to map prompt-weight pairs, which can produce customized weights for novel
tasks by processing unlabeled task prompts. Notably, our approach can transform _unlabeled task_
_prompts_ directly to LoRA weights update _in seconds_ . This prompt-to-weight paradigm, which can
generate task-specific weights _without further tuning_, sheds lights on a promising new direction for
efficient LLM and VLLM customization.


Previous research [ 50, 57, 58 ] and our approach demonstrate that neural network weights can be
effectively synthesized. It appears that network parameters can be viewed as a new form of data
modality. To excavate this emerging field’s potential, several challenges remain to be tackled. First,
scaling parameter generation to larger models (7B-70B parameters) requires novel architectural
and algorithmic advances. Second, leveraging existing pre-trained checkpoints from the Internet
could enhance the practicality of parameter generators. Third, generating structurally diverse models
adaptable to various hardware configurations would improve deployment flexibility.


**Ackonwledgement.** We sincerely appreciate PERSON_15, PERSON_16, PERSON_17, PERSON_18, PERSON_19, PERSON_20, PERSON_21, PERSON_22, PERSON_23, and PERSON_24 for valuable
discussions and feedbacks during this work.


**References**


[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. _arXiv preprint arXiv:2303.08774_, 2023.


[[2] PERSON_25 2023. PERSON_25-2023/code-74k-sharegpt. Code-74k-ShareGPT, 2023.](URL_2)


[3] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh
Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based
formalisms. In _NAACL_, 2019.


[4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,
Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. _arXiv preprint arXiv:2502.13923_,
2025.


[5] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical
commonsense in natural language. In _AAAI_, 2020.


[6] Léon Bottou et al. Stochastic gradient learning in neural networks. _NeuroNîmes_, 1991.


[7] Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation.

`[https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)`, 2023.


[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.


[9] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In
_NAACL_, 2019.


[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ORGANIZATION_5 reasoning
challenge. _arXiv preprint arXiv:1803.05457_, 2018.
  

--- Page Break ---

[11] PERSON_26, PERSON_27, PERSON_28, PERSON_29, PERSON_30, PERSON_31, PERSON_32, PERSON_33, PERSON_34, PERSON_35, et al. Training verifiers to solve math word problems. _arXiv preprint ARXIV_ID_1_, 2021.


[12] ORGANIZATION_6. ORGANIZATION_6.v_2 — ORGANIZATION_6.v_3,, 2022. [Online; accessed 8-December-2022].


[13] PERSON_36, PERSON_37, PERSON_38, and PERSON_39. Bert: Pre-training of deep bidirectional transformers for language understanding. In _CONFERENCE_1_, 2019.


[[14] COMPANY_6. DATASET_1. DATASET_1, 2023.](URL_3)


[[15] COMPANY_6. DATASET_2. DATASET_2.v_2, 2024.](URL_4)


[16] PERSON_41 and PERSON_42. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _CONFERENCE_2_, 2016.


[[17] COMPANY_7. DATASET_3. DATASET_3.v_2, 2024.](URL_5)


[18] PERSON_43, PERSON_44, PERSON_45, PERSON_46, PERSON_47, PERSON_48, PERSON_49, PERSON_50, PERSON_51, PERSON_52, et al. The llama
3 herd of models. _arXiv preprint ARXIV_ID_2_, 2024.


[19] PERSON_53. Practical variational inference for neural networks. In _CONFERENCE_3_, 2011.


10


[20] PERSON_54 and PERSON_55. Mamba: Linear-time sequence modeling with selective state spaces. In
_CONFERENCE_4_, 2024.


[21] PERSON_56, PERSON_57, PERSON_58, PERSON_59, PERSON_60, PERSON_61, PERSON_62, PERSON_63, PERSON_64, PERSON_65, et al. Deepseek-r1: Incentivizing reasoning capability in
llms via reinforcement learning. _arXiv preprint ARXIV_ID_3_, 2025.


[22] PERSON_66, PERSON_67, and PERSON_68. Lora+: Efficient low rank adaptation of large
models. In _CONFERENCE_2_ . PUBLISHER_1, 2024.


[23] PERSON_69, PERSON_70, PERSON_71, PERSON_72, PERSON_73, PERSON_74, PERSON_75, and PERSON_76. Measuring mathematical problem solving with the math dataset. In
_CONFERENCE_3_, 2021.


[24] PERSON_77, PERSON_78, PERSON_79, PERSON_80, PERSON_81, PERSON_82, PERSON_83, et al. Lora: Low-rank adaptation of large language models. In _CONFERENCE_5_, 2022.


[25] PERSON_84, PERSON_85, PERSON_86, PERSON_87, PERSON_88, PERSON_89, PERSON_90, PERSON_91, PERSON_92, and PERSON_93. DATASET_4: Holistic and contamination
free evaluation of large language models for code. In _CONFERENCE_5_, 2024.


[26] PERSON_94, PERSON_14, PERSON_2, PERSON_6, PERSON_95, PERSON_96, and PERSON_12.
Conditional lora parameter generation. _arXiv preprint ARXIV_ID_4_, 2024.


[27] PERSON_97, PERSON_2, PERSON_98, PERSON_14, and PERSON_99.
Oral: Prompting your large-scale loras via conditional recurrent diffusion. _arXiv preprint_
_ARXIV_ID_5_, 2025.


[28] PERSON_100, PERSON_101, and PERSON_102. Vilt: Vision-and-language transformer without
convolution or region supervision. In _CONFERENCE_2_, 2021.


[29] PERSON_103 and PERSON_104. Auto-encoding variational bayes. _arXiv preprint ARXIV_ID_6_,
2013.


[30] PERSON_105, PERSON_106, and PERSON_107. Vera: Vector-based random
matrix adaptation. In _CONFERENCE_5_, 2024.


[31] PERSON_108 et al. Learning multiple layers of features from tiny images. 2009.


[32] PERSON_109, PERSON_110, PERSON_111, PERSON_112, PERSON_113, PERSON_114, and
PERSON_115. Spoc: Search-based pseudocode to code. In _CONFERENCE_3_, 2019.


[[33] COMPANY_8. DATASET_5., 2023.](URL_6)


[34] PERSON_21, PERSON_116, and PERSON_117. Text-to-model: Text-conditioned neural network diffusion
for train-once-for-all personalization. _arXiv preprint ARXIV_ID_7_, 2024.


[35] PERSON_118, PERSON_119, PERSON_120, PERSON_121, PERSON_122, PERSON_123, PERSON_124,
PERSON_125, PERSON_126, PERSON_127, et al. Deepseek-v3 technical report. _arXiv preprint_
_ARXIV_ID_8_, 2024.


[36] PERSON_128, PERSON_129, PERSON_130, PERSON_131, PERSON_132,
PERSON_133, and PERSON_134. Dora: Weight-decomposed low-rank adaptation. In
_CONFERENCE_2_, 2024.


[37] PERSON_135 and PERSON_136. A decade’s battle on dataset bias: Are we there yet? In _CONFERENCE_5_,
2025.


[38] PERSON_137, PERSON_138, PERSON_139, PERSON_140, PERSON_141, PERSON_142, PERSON_143,
PERSON_144, PERSON_145, and PERSON_146. DATASET_6: Evaluating mathematical
reasoning of foundation models in visual contexts. In _CONFERENCE_5_, 2024.


[39] PERSON_147, PERSON_148, PERSON_149, PERSON_150, PERSON_151, PERSON_152, PERSON_153, PERSON_154,
PERSON_155, and PERSON_156. DATASET_7: Empowering code large language models
with DATASET_8. In _CONFERENCE_5_, 2024.


11


[40] PERSON_157, PERSON_158, PERSON_159, and PERSON_160. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In _CONFERENCE_6_, 2018.


[[41] COMPANY_9. DATASET_9. DATASET_9.v_2, 2023.](URL_7)


[42] PERSON_161, PERSON_162, PERSON_163, PERSON_164, and PERSON_165. Learning
to learn with generative models of neural network checkpoints. _arXiv preprint ARXIV_ID_9_,
2022.


[43] PERSON_166, PERSON_167, and PERSON_168. Glove: Global vectors for
word representation. In _CONFERENCE_6_, 2014.


[[44] COMPANY_10. DATASET_10. DATASET_10.v_2, 2023.](URL_8)


[45] PERSON_169, PERSON_170, PERSON_171, PERSON_172, PERSON_173, PERSON_174, PERSON_175,
PERSON_176, and PERSON_177. Exploring the limits of transfer learning with a unified
text-to-text transformer. _JOURNAL_1_, 21(140):1–67, 2020.


[46] PERSON_178 and PERSON_179. MODEL_4: Sentence embeddings using siamese bertnetworks. In _CONFERENCE_6_, 2019.


[47] PERSON_180, PERSON_181, PERSON_182, PERSON_183, and PERSON_184.
High-resolution image synthesis with latent diffusion models. In _CONFERENCE_7_, 2022.


[48] PERSON_185, PERSON_186, PERSON_187, PERSON_188, PERSON_189, PERSON_190, PERSON_191,
PERSON_192, and PERSON_193. Hyperdreambooth: Hypernetworks for fast
personalization of text-to-image models. In _CONFERENCE_7_, 2024.


[49] PERSON_194, PERSON_195, PERSON_196, and PERSON_197. DATASET_18: An
adversarial winograd schema challenge at scale. In _CONFERENCE_8_, 2020.


[50] PERSON_9, PERSON_198, PERSON_199, and PERSON_10. Hyperrepresentations for pre-training and transfer learning. In _CONFERENCE_3_, 2022.


[51] PERSON_9, PERSON_198, PERSON_199, and PERSON_10. Hyperrepresentations for pre-training and transfer learning. _arXiv preprint ARXIV_ID_10_, 2022.


[52] PERSON_9, PERSON_200, and PERSON_10. Towards scalable and versatile
weight space learning. _CONFERENCE_2_, 2024.


[53] PERSON_9, PERSON_200, and PERSON_10. Towards scalable and versatile
weight space learning. _arXiv preprint ARXIV_ID_11_, 2024.


[54] PERSON_201, PERSON_202, PERSON_203, PERSON_204, PERSON_205, PERSON_206, PERSON_207, and
PERSON_208. Math-llava: Bootstrapping mathematical reasoning for multimodal large language
models. In _CONFERENCE_6_, 2024.


[[55] ORGANIZATION_7. DATASET_11. DATASET_11.v_2, 2023.](URL_9)


[56] PERSON_209, PERSON_210, PERSON_211, and PERSON_212. Dylora: Parameter-
efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In _CONFERENCE_9_,
2023.


[57] PERSON_14, PERSON_2, PERSON_213, PERSON_214, PERSON_215, PERSON_95, PERSON_216, PERSON_217,
PERSON_135, and PERSON_12. Neural network diffusion. _arXiv preprint ARXIV_ID_12_,
2024.


[58] PERSON_14, PERSON_2, PERSON_6, PERSON_9, PERSON_13, and PERSON_12.
Recurrent diffusion for large-scale parameter generation. _arXiv preprint ARXIV_ID_13_,
2025.


[59] PERSON_218, PERSON_219, PERSON_220, PERSON_221, PERSON_222, PERSON_223, PERSON_224, and
PERSON_225. Measuring multimodal mathematical reasoning with math-vision dataset. In
_CONFERENCE_3_, 2024.


12


[60] PERSON_226, PERSON_227, PERSON_228, PERSON_229, PERSON_230, PERSON_231,
and PERSON_142. Self-instruct: Aligning language models with self-generated instructions.
In _CONFERENCE_9_, pages 13484–13508, 2023.


[61] PERSON_232, PERSON_233, PERSON_234, PERSON_235, PERSON_236, PERSON_237, PERSON_238,
PERSON_239, et al. Chain-of-thought prompting elicits reasoning in large language models. In
_CONFERENCE_3_, 2022.


[62] PERSON_240, PERSON_241, PERSON_242, PERSON_243, PERSON_244, PERSON_245, PERSON_246,
PERSON_247, PERSON_248, PERSON_249, et al. Qwen2. 5 technical report. _arXiv preprint_
_ARXIV_ID_14_, 2024.


[63] PERSON_250, PERSON_251, PERSON_252, PERSON_253, and PERSON_254. DATASET_12: Combining tool-use large language models with multi-perspective data augmentation for mathematical
reasoning. _arXiv preprint ARXIV_ID_15_, 2024.


[64] PERSON_255, PERSON_256, PERSON_257, PERSON_258, and PERSON_197. DATASET_14: Can a
machine really finish your sentence? In _CONFERENCE_9_, 2019.


[65] PERSON_213, PERSON_214, and PERSON_135. Understanding bias in large-scale visual datasets. In
_CONFERENCE_3_, 2024.


13


We organize our appendix as follows.


**Hyper-parameter Settings**


Section A.1: Training Recipe.


Section A.2: Description of Datasets.


Section A.3: Detailed Structure of Hyper-Convolutional Decoder.


**Additional Experiment Results**


Section B.1: More Results of Common Sense Reasoning, Math, and Coding.


Section B.2: Inference Efficiency Analysis.


Section B.3: More Ablation Studies.


**A** **Hyper-parameter Settings**


**A.1** **Training Recipe**


In this section, we provide details of our training recipe and various hyper-parameter settings. We
incorporate multiple tasks in language models, each involves different foundation model sizes,
different generator architecture, and training schedules. We report settings for every task in Table 7.


training setting common sense coding math multimodal

batch size 128 (0.5B) 128 (1.5B), 48 (7B) 128 (1.5B), 48 (7B) 64 (3B)

optimizer AdamW AdamW AdamW AdamW
learning rate 3e-5 3e-5 3e-5 3e-5
length of prompt batch 128 16 32 16
training step 5,000 5,000 5,000 5,000
weight decay 0.1 0.1 0.1 0.1
max grad norm 1.0 1.0 1.0 1.0
noise aug. amplitude 1e-4 1e-4 1e-4 1e-4
Table 7: Training recipe for different tasks in Section 3.2 and Section 3.3.


**Length of prompt batch.** It has been introduced in Section 2.3 that every parameter is grouped with
certain amount of texts each iteration. Due to the length of prompt in different datasets varies and
induces variable training costs, the length of prompt batch also varies.


**A.2** **Description of Datasets**


In this section, we introduce the datasets used in the paper, including those for common sense
reasoning, math, coding, and multimodal tasks.
**Common sense reasoning. DATASET_15 dataset [ 10 ] contains grade-school level, multiple-choice science
questions and is splited into easy and challenge sets. DATASET_16 [ 40 ] aims to promote research in
advanced question-answering with salient facts summarized as an open book. DATASET_17 [ 5 ] focuses
on everyday situations with a preference for atypical solutions. DATASET_14 [ 64 ] instructs models
to select from choices that best finish the sentence among ground truth and an adversarial set of
machine-generated wrong answers. DATASET_18 [ 49 ] features a fill-in-a-blank task with binary
options for commonsense reasoning questions. DATASET_19 [ 9 ] is a question answering dataset for yes/no
questions containing various factual problems.
**Coding. DATASET_8 [ 39 ] contains evolutionary prompts tailored for code-related tasks and
incorporates code debugging and time-space complexity constraints. DATASET_3 [ 17 ] is a
dataset of code problems and solutions generated using COMPANY_7.v_2 synthetic data generation platform.
DATASET_2.v_3 [ 15 ] is a cleaned Python dataset covering instructional tasks. DATASET_21 [ 2 ]
consists of conversations along with detailed Python code explanations. It is generated using MODEL_1,
MODEL_2 etc. DATASET_22 [ 12 ] presents solutions to the same task in as many different languages as
possible, to aid a person with a grounding in one approach to a problem in learning another. DATASET_1.v_3
[ 14 ] primarily focuses on instructional tasks in Python, tokenized specifically for the
MODEL_3 architecture. It is a blend of MODEL_2 generated content, custom codes, behavioral approaches
and tasks. DATASET_24 [ 7 ] dataset is generated by the techniques in [ 60 ], with some modifications.
**Math. DATASET_25 [ 23 ] consists of problems from math competitions, including the COMPETITION_1,
COMPETITION_2, COMPETITION_3, and more. DATASET_10.v_2 [ 44 ] mathematical questions and with
corresponding step-by-step solutions. DATASET_26 [ 3 ] is a large-scale dataset of math word problems that
are densely annotated with operation programs. DATASET_11 [ 55 ] is a augmented dataset with MODEL_2.
DATASET_12.v_2 [ 63 ] is a meta-evaluation dataset derived from the DATASET_13 [ 63 ] benchmark, intended to
assess the ability of LLMs to judge free-form mathematical solutions. DATASET_9.v_2 [ 41 ] prioritizes
reasoning and explanatory problem-solving over provided answers.
**Multimodal. DATASET_27 [ 54 ] consists 40K images from 24 datasets and 360K question-answer
pairs. It is used to enhance the multimodal mathematical reasoning capabilities of MLLMs.


**A.3** **Detailed Structure of Hyper-Convolutional Decoder**


**Details of condition extractor.** As introduced in Section 3, we use MODEL_4 [ 46 ] as our
condition extractor in default (MODEL_5 specifically). Since MODEL_6 supported sequence
length is only 512, for longer sequences, we need to preprocess the input sequences. Specifically, we
first pad the sequence to the length that can be divided by 512, slice it into multiple sub-strings, and
encode the sub-strings respectively. The input length for different tasks is in the table below.


common sense coding math multimodal
length 384 26624 4608 1536

Table 8: Model architectures for different tasks.


**Details of hyper-convolutional decoder.**


In this part, we delve into hyper-convolutional decoder’s inner architecture introduced in Section 2.5.
The decoder consists of multiple cascading hyper-convolutional blocks, each containing 5 2D
convolution modules. Specifically, we divide convolutions into three categories: i) **width convolution**
that operates on ( _C, L_ ) dimension, ii) **height convolution** that operates on ( _L, N_ ) dimension) iii)
**layer-wise convolution** that on ( _N, L_ ) dimension), with notations Conv _W_, Conv _H_, and Conv _L_ . In
the above convolution modules, the input tensor is transposed to the shape that specified dimensions
act as feature maps, the remaining dimension act as channel dimension like conventional convolution.
Each hyper-convolutional block consists of two Conv _W_, two Conv _H_ and one Conv _L_ . Given this, the
forward operation of a hyper-convolutional block can be formulated as:


_c_ _[l]_ _W_ [=][ Conv] [1] _H_ [(][Conv] [1] _W_ [(] _[c]_ _[l][−]_ [1] [));]



_c_ _[l]_ _H_ [=][ Conv] [2] _W_ [(][Conv] [2] _H_ [(] _[c]_ _[l][−]_ [1] [));]

_c_ _[l]_ = Conv _L_ ( _[c]_ _W_ _[l]_ [+] _[ c]_ _[l]_ _H_ [+] _[ b]_ ) _,_
3



(5)



where _c_ _[l]_ is hidden state output by the _l_ th layer, _c_ [0] is prompt embedding encoded by the condition
extractor, and _b_ is learnable bias. Through this process, it transforms the input with shape of

[ _B, N, L, C_ ] to [ _B, N_ _[′]_ _, L_ _[′]_ _, C_ _[′]_ ].
**Model architecture used in different tasks.** In this part, we show the architecture of hyperconvolutional decoders. We use the three element tuple ( _N, L, C_ ) to represent decoder structure since
it reflects input conditions’ changes in the network. Also, _B_ dimension is omitted since it doesn’t
affect the model architecture. Note that for math and coding tasks, we first project the _L_ dimension to
1000 to reduce overwhelming memory costs induced by giant convolution kernels.


**A.4** **Details of trained checkpoints collection**


In this section, we discuss how we collect checkpoints in detail. It is noteworthy that all checkpoint
collection process go through two phases: i) Pretraining on the target dataset for specified steps. ii)
Fine-tuning on the target dataset for certain additional steps, while saving a checkpoint at each step.


Therefor, the essence of checkpoint collection is the pretrain and fine-tune phase’s learning rate,
training steps, number of samples, and batch size. Note that except for learning rate and training steps,
all other hyper-parameters are kept the same. Detailed settings are in Table 9. For those datasets
contain less samples than the number specified below, we use the entire dataset for training.


15


|task|foundation model|channel|
|---|---|---|
|common sense|0.5B|(128, 384, 384)_→_(128, 200, 300)_→_(128, 100, 256)<br>(256, 50, 200)_→_(512, 50, 200)_→_(1024, 25, 200)<br>(1024, 10, 200)_→_(2048, 10, 200)_→_(4296, 8, 128)|
|coding|1.5B|(32, 1000, 384)_→_(64, 500, 300)_→_(256, 500, 300)_→_(512, 125, 300)<br>(1024, 64, 256)_→_(2048, 32, 256)_→_(4508, 16, 256)|
|coding|7B|(16, 1000, 384)_→_(64, 500, 384)_→_(256, 125, 400)_→_(512, 64, 400)<br>(1024, 64, 400)_→_(2048, 32, 400), (4928, 16, 512)|
|math|1.5B|(16, 1000, 384)_→_(64, 500, 300)_→_(256, 125, 300)_→_(512, 64, 300)<br>(1024, 64, 256)_→_(2048, 32, 256)_→_(4508, 16, 256),|
|math|7B|(16, 1000, 384)_→_(64, 500, 384)_→_(256, 125, 400)_→_(512, 64, 400)<br>(1024, 64, 400)_→_(2048, 32, 400), (4928, 16, 512)|
|multimodal|3B|(16, 1536, 384)_→_(64, 500, 300)_→_(256, 125, 300)<br>(1024, 64, 300)_→_(2048, 16, 256)_→_(7308, 16, 256)|


Table 8: Model architectures for different tasks.


pretrain finetune
task \ setting
lr. training step batch size #num. samples lr. training step

common sense 1e-4 75 32 5000 1e-5 50

coding 1e-4 4000 64 10000 1e-6 100

math 1e-4 4000 64 10000 1e-6 100

multimodal 1e-4 8000 64 100000 1e-6 200


Table 9: Details for checkpoint collection.


**B** **Additional Experiment Results**


**B.1** **More Results of Common Sense Reasoning, Math, and Coding**


In this section, we delve into details about each group of training LoRAs’ performance on test sets,
report their performance and discuss further findings.

|train set \ test set|DATASET_15.v_2 DATASET_16 DATASET_15.v_3 DATASET_17 DATASET_14 DATASET_19 DATASET_18|
|---|---|
|training LoRA of DATASET_15.v_2<br>of DATASET_16<br>of DATASET_15.v_3<br>of DATASET_17<br>of DATASET_14<br>of DATASET_19<br>of DATASET_18<br>MODEL_7.v_2|59.4<br>46.2<br>46.7<br>80.4<br>30.6<br>44.6<br>52.2<br>64.3<br>38.4<br>53.4<br>56.1<br>29.0<br>1.3<br>-<br>57.2<br>38.2<br>40.7<br>65.9<br>46.7<br>23.6<br>-<br>27.9<br>27.0<br>24.7<br>66.2<br>24.4<br>9.2<br>50.3<br>57.2<br>43.2<br>41.0<br>40.4<br>23.4<br>0.5<br>52.8<br>fail<br>fail<br>52.0<br>fail<br>fail<br>22.11<br>fail<br>18.6<br>26.8<br>19.1<br>0.1<br>3.8<br>1.6<br>50.5<br>54.8<br>16.6<br>38.3<br>16.6<br>26.5<br>37.0<br>50.2|
|average of training LoRAs<br>PROJECT_1|37.5<br>30.2<br>39.5<br>40.5<br>22.4<br>13.5<br>38.8<br>68.6<br>40.8<br>51.6<br>87.9<br>25.9<br>44.9<br>50.0|



Table 10: More results for common sense reasoning task. Red marks full-shot results, gray shows
the average of training LoRAs, and green marks the results of PROJECT_1. PROJECT_1 not only consistently
surpasses average of training LoRAs, but also outperform their full-shot performance in most cases.


**Common sense reasoning.** In this part, we show training LoRAs’ performance on each test set in
Table 10. Each row specifies the dataset LoRA is trained on, and each column denotes the dataset
used for testing. Consequently, the diagonal elements are full-shot results of training LoRAs, which
are marked in red . We also incorporate their average accuracy (exclude diagonal elements, marked
in gray ), MODEL_7.v_2’s performance, and PROJECT_1’s zero-shot results (marked in green ).


It can be observed that: i) original LoRAs generally perform poorly on novel datasets, _e.g._, LoRAs
tuned on DATASET_19 fail (accuracy=0.0) on half of zero-shot datasets, this may because training on one
specific dataset will fit the parameters for this dataset (binary True/False in DATASET_19’s case) and limit


16

their generality (multiple choices question of other datasets). ii) PROJECT_1 even outperforms training
LoRAs’ full-shot performances in most cases, showing it has incredible zero-shot ability with great
efficiency. These findings further illustrates that: _fitting parameters for certain data may lack_
_generality, learning the correlations between data and parameters may be better._
**Coding.** In this part, we elaborate on training LoRAs’ performance on coding datasets, along with
their average (marked in gray ), MODEL_7.v_3/MODEL_7.v_4, and PROJECT_1’s performance (marked in green ).

|test set \ train set|DATASET_21.v_2<br>GPT|DATASET_8.v_2<br>Instruct|DATASET_3.v_3<br>Assistant|DATASET_2.v_4<br>Python|DATASET_22.v_2<br>Rosetta|DATASET_1.v_4<br>Python|DATASET_24.v_2<br>Alpaca|MODEL_7.v_3<br>1.5B|train set avg.|PROJECT_1|
|---|---|---|---|---|---|---|---|---|---|---|
|pass@1<br>pass@5<br>pass@10|28.8<br>46.2<br>52.9|40.0<br>53.4<br>56.4|14.0<br>27.8<br>35.0|9.8<br>19.1<br>23.8|17.6<br>28.6<br>33.2|13.7<br>25.5<br>29.9|23.2<br>31.0<br>34.1|14.7<br>26.5<br>32.3|17.6<br>28.6<br>33.2|32.7<br>55.3<br>64.1|


testset: DATASET_28, average improvement: pass@1 = **15.1**, pass@5 = **26.7**, pass@10 = **30.9**


(a) pass@k (k = 1, 5, 10) scores of foundation LLM, original and generated LoRA for MODEL_7.v_3 on DATASET_28.
PROJECT_1 shows improves largely over its training data and base MODEL_7, validating its effectiveness.

|test set \ train set|DATASET_21.v_2<br>GPT|DATASET_8.v_2<br>Instruct|DATASET_3.v_3<br>Assistant|DATASET_2.v_4<br>Python|DATASET_22.v_2<br>Rosetta|DATASET_1.v_4<br>Python|DATASET_24.v_2<br>Alpaca|MODEL_7.v_4<br>7B|train set avg.|PROJECT_1|
|---|---|---|---|---|---|---|---|---|---|---|
|pass@1<br>pass@5<br>pass@10|22.4<br>28.4<br>30.8|23.2<br>28.2<br>30.3|24.6<br>32.7<br>35.8|fail<br>fail<br>fail|fail<br>fail<br>fail|fail<br>fail<br>fail|fail<br>fail<br>fail|34.1<br>41.6<br>43.8|13.0<br>16.4<br>17.6|33.4<br>42.1<br>46.0|


testset: DATASET_4.v_2, average improvement: pass@1 = **20.3**, pass@5 = **25.7**, pass@10 = **28.4**


(b) PROJECT_1 constantly present promising results at more complex benchmarks ( _i.e._, DATASET_4.v_2), even when
over half of its training LoRAs fail (pass@k = 0.0) on this dataset.
Table 11: PROJECT_1 surpasses the average pass@k score of training data on zero-shot coding benchmarks.


Empirical results in Table 11 indicates that: i) our approach is able to generalize to complex real-
world problems, generating parameters for zero-shot benchmarks with promising results (reflected
in improvement over training data). ii) PROJECT_1’s good performance on the MODEL_7.v_4 group, despite poor
performance of training LoRAs (pass@k=0.0), stressing that _our method is not simply memorizing_
_seen parameters, but learn to establish data-parameter mappings and adapt LLMs for novel datasets._

|test set \ train set|DATASET_10.v_4|DATASET_9.v_4|DATASET_11|DATASET_12.v_2|DATASET_25.v_2|DATASET_26.v_2|MODEL_7|train set avg.|PROJECT_1|
|---|---|---|---|---|---|---|---|---|---|
|DATASET_29<br>DATASET_30|68.6<br>30.0|35.4<br>1.5|68.3<br>30.2|22.8<br>7.2|31.0<br>16.5|31.4<br>3.5|64.4<br>29.3|42.9<br>14.8|66.3<br>23.9|


average accuracy improvement: **23.4** on DATASET_29, **9.1** on DATASET_30.


(a) Even with some low-performing training LoRAs ( _i.e._, accuracy less than 50% of base MODEL_7), PROJECT_1 still
maintains good zero-shot performance, showing the drag-and-drop ability to fit LLMs for novel datasets.

|test set \ train set|DATASET_10.v_4|DATASET_9.v_4|DATASET_11|DATASET_12.v_2|DATASET_25.v_2|DATASET_26.v_2|MODEL_7.v_4|train set avg.|PROJECT_1|
|---|---|---|---|---|---|---|---|---|---|
|DATASET_29|88.3|50.7|68.3|61.1|88.3|60.7|81.2|65.9|83.1|


average accuracy improvement: **17.2** on DATASET_29.


(b) Using larger MODEL_7.v_4, PROJECT_1 continues to drag-and-drop it, indicating our method has good scalability.
Table 12: PROJECT_1 continues to work well on math tasks, showing our method has broad applicability.


**Math.** In this part, we report more results for math experiments in Table 12. Similar to common sense
reasoning and coding tasks, PROJECT_1 continues to work well on math datasets, improving largely over
the average accuracy of training data on zero-shot benchmarks. This showcases its drag-and-drop
ability has wide application scenarios. Also, results on MODEL_7.v_4 prove its promising scalability.


**B.2** **Inference Efficiency Analysis**


In this part,we further analyze PROJECT_1’s efficiency by presenting its memory usage and inference time
for generating various LLMs on respective tasks. We show the cost of generating one single model
on a COMPANY_11 HARDWARE_1 in Table 13.


17


metrics common sense math coding multimodal


0.53 (1.5B) 0.70 (1.5B)
time (second) 0.11 0.61
0.55 (7B) 0.73 (7B)


15.43 (1.5B) 19.17 (1.5B)
memory cost (GB) 9.59 20.31
16.22 (7B) 20.48 (7B)


Table 13: Inference time and memory cost for different LLMs generation. All metrics are measured
on a single COMPANY_11 HARDWARE_1. The time and memory is the cost to generate a single model.


**B.3** **More Ablation Studies**


**Can answer serve as condition in more complex scenarios?** In Section 3.4, we’ve explored
different condition types and come up with the conclusion: _simple, identical answers will limit the_
_diversity of conditions._ In this part, we explore using more complex answers from math datasets in
Section 3.3 as conditions for building condition-parameter pairs and training PROJECT_1.



From results in Table 14, we can observe that more
complex, diverse answers can lead to better performance. This may because their diversity is able to
provide PROJECT_1 with a comprehensive view of conditionparameter mapping. However, as answers are typically much longer than prompts in math and coding
datasets due to problem complexity, we still recommend to use prompts as conditions for PROJECT_1.


18



Table 14: Answer in math task can serve as
conditions, but prompts still work better.